from pyspark.sql import SparkSession
from pyspark.sql.functions import col

spark = SparkSession.builder.appName("Notebook Pipeline").getOrCreate()

df = spark.read.json("../data/input/customers.json")
df_clean = df.filter(col("name").isNotNull())
df_clean.write.mode("overwrite").partitionBy("country").parquet("../data/output/customers")

spark.stop()
